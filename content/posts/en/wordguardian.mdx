---
title: "The WordGuardian Project"
date: "2025-06-02"
summary: "Over the past month, Iâ€™ve immersed myself in a personal project that was meant to be a game, an experiment, and maybe a bit of a protest: **WordGuardian**. A website for playing with absurd, real, and invented definitions. But also a window into Wikidataâ€™s semantic richness and a cold shower on the reality of building an indie web in 2025."
image: "/images/wordguradian-post.png" 
categories: ["general", "technology", "game", "language", "wikidata", "astro", "webindie"]
slug: "wordguardian"
---

# The Project  
**Version 1.0**  
âœï¸ AI-assisted: This article was written with the help of ChatGPT (OpenAI), based on original sketches and ideas by **Francesc LÃ³pez MariÃ³**.

Over the past month, Iâ€™ve immersed myself in a personal project that was meant to be a game, an experiment, and maybe a bit of a protest: **WordGuardian**.

A site for playing with absurd, real, and invented definitions. But also a window into the semantic richness of Wikidata and a cold reality check about building an indie website in 2025.

## ğŸ§© Objective  
I wanted to create a game where users guessed plausible definitions, either automatically generated or extracted.  

**But the journey was anything but simple.**  
I used Astro and Preact to avoid FOUC and keep it responsive, and I worked with massive Wikidata dumps, decompressed with `pbzip2`, `zstd`, and parsed using parallel **Rust** scripts.

## âš™ï¸ Stack & Techniques  

- **Frontend**: Astro, Preact, Shoelace, Vanilla Extract  
  > *Astro turned out to be a hidden gem.* It lets you combine components from different frameworks and enables efficient **server-side rendering (SSR)**. This helps search engines index the content properly â€”a key difference if you care about SEO in 2025. Also, exposing a small server API (via `/api/xyz` endpoints) is simple and clean, no need for a full backend.  
- **Backend**: Node.js for scraping; parallel Rust scripts for parsing Wikidata  
- **Processing**: Wikidata dumps using p31, p279, p1629... a whole world of data  
- **NLP**: distractor generation with Groq, heuristics using embeddings.
- **Other**: Load without *FOUC* (*Flash of Unstyled Content*) â€”making sure the page doesnâ€™t flash ugly before styles kick in. Minimal optimization, and as always, lots of console logs.

I also relied on the [**Wiktextract**](https://github.com/tatuylonen/wiktextract) project by **Tatu YlÃ¶nen**, and the dataset from [**Kaikki.org**](https://kaikki.org), which offers Wiktionary content in a structured, usable way. Without it, building a solid lexical base would have taken much longer.

## ğŸ’¡ What I Learned  

- **Wikidata is a treasure**, but navigating it without a map is a nightmare: items, properties, lexemes, glosses, cross-references... and **SPARQL** queries that look like arcane rituals.  
- Tools I discovered:
  - [Wikidata Toolkit](https://github.com/Wikidata-Toolkit/Wikidata-Toolkit)  
  - [KGTK Search](https://github.com/usc-isi-i2/kgtk-search)  
  - [Wikibase CLI](https://github.com/maxlath/wikibase-cli/tree/main)  
  - [wikidata-filter](https://github.com/alexkreidler/wikidata-filter)  
  - [SPARQL](https://en.wikipedia.org/wiki/SPARQL)
- Wikidata dumps often come in **JSONL** (JSON Lines) format: a file where each line is a standalone JSON object. Great for streaming â€”you donâ€™t need to load everything into memory. But with 90 GB... even the best strategy requires patience.
- **Compression formats matter**:  
  - `bz2` is common in official dumps, but *painfully* slow to decompress.  
  - `zstd` is **much faster**, especially with multiple threads. With `pzstd -d -p10` you can speed up decompression, although your own code becomes the bottleneck ğŸ˜…
- Generating plausible distractors is not trivial. Doing it with Groq was fun but inconsistent.  
- Making fake but believable definitions is tricky. You need precise prompting with Groq or HuggingFace.  
- **Dumps are not for the faint of heart.** Especially when theyâ€™re 90 GB.
- I also learned to wrap small NLP services in Docker, exposing local APIs with ease. Using a lightweight image like `python:3.11-slim` and a `requirements.txt`, I quickly deployed a service for semantic comparisons and phrase similarity. Perfect for local tests without dependency chaos.
- I explored [**Common Crawl**](https://commoncrawl.org/) to URLs from the public web. Although I didnâ€™t end up using it, I built a script to filter the links and create a custom corpus. The idea was to use it to generate or validate questions with real content. Itâ€™s on hold... for now.

## ğŸ“‰ Monetization? Yikes...

I had hoped to at least cover the hosting with ads. But the numbers speak for themselves:

- 462 visits  
- 22 active users  
- â‚¬0.00 revenue  
- An average of 7 minutes per user... but no clicks

> In 2025, if you donâ€™t have TikTok, a newsletter, or a million page views, ads wonâ€™t even buy you coffee.  
> Making a website isnâ€™t enough. You have to be a showman, have money, or do daily marketing.

## ğŸ¯ So, what now?

Iâ€™m not killing it.  
**WordGuardian will live on** as an experimental project and a small tribute to words and to Wikidata.  

Maybe it will evolve into an educational app. Maybe itâ€™ll become the base for a new game.  
Or maybe itâ€™ll stand as a monument to the time spent on an idea that still makes me smile.

Iâ€™m also toying with the idea of **exploring how to use Wikidataâ€™s ontology to generate questions by topic** â€”science, history, pop culture, etc.  
Using subclass (`p279`) and instance (`p31`) hierarchies to build *thematic levels* or *topic-based quizzes*, making the game a bit less â€œpsychedelicâ€ and a bit more navigable.  

ğŸ‘‰ Try it: [https://eurekatop.com/wordguardian](https://eurekatop.com/wordguardian)  
ğŸ“– Open source (coming soon on GitHub)  

A project by [**mutiitu.com**](https://www.mutiitu.com)

Â© Francesc LÃ³pez MariÃ³, 2025
